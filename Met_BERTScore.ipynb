{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9bf225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipywidgets) (9.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: wcwidth in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/jnardosp/miniconda3/envs/hc_experiment/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe502185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Gemini</th>\n",
       "      <th>GPT - 4</th>\n",
       "      <th>Deepseek</th>\n",
       "      <th>Humano</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fecha apertura: 06/01/2021 08:08 Fecha: 06/01/...</td>\n",
       "      <td># Historia Clínica\\n\\n## I. Información Genera...</td>\n",
       "      <td># Consulta Médica - Teleconsulta\\n\\n## Informa...</td>\n",
       "      <td>**Fecha apertura:** 06/01/2021 08:08  \\n**Fech...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fecha apertura: 18/03/2021 10:28 Fecha: 18/03/...</td>\n",
       "      <td># Historia Clínica\\n\\n---\\n\\n## I. Información...</td>\n",
       "      <td># Seguimiento de Enfermería - Hemodinamia Adul...</td>\n",
       "      <td>**Fecha apertura:** 18/03/2021 10:28  \\n**Fech...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fecha: 18/03/2021 15:18 - Ubicación: HEMODINAM...</td>\n",
       "      <td># Historia Clínica\\n\\n---\\n\\n## I. Información...</td>\n",
       "      <td># Resumen de Atención - Hemodinamia Adultos\\n\\...</td>\n",
       "      <td>**Fecha:** 18/03/2021 15:18  \\n**Ubicación:** ...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Original  \\\n",
       "0  Fecha apertura: 06/01/2021 08:08 Fecha: 06/01/...   \n",
       "1  Fecha apertura: 18/03/2021 10:28 Fecha: 18/03/...   \n",
       "2  Fecha: 18/03/2021 15:18 - Ubicación: HEMODINAM...   \n",
       "\n",
       "                                              Gemini  \\\n",
       "0  # Historia Clínica\\n\\n## I. Información Genera...   \n",
       "1  # Historia Clínica\\n\\n---\\n\\n## I. Información...   \n",
       "2  # Historia Clínica\\n\\n---\\n\\n## I. Información...   \n",
       "\n",
       "                                             GPT - 4  \\\n",
       "0  # Consulta Médica - Teleconsulta\\n\\n## Informa...   \n",
       "1  # Seguimiento de Enfermería - Hemodinamia Adul...   \n",
       "2  # Resumen de Atención - Hemodinamia Adultos\\n\\...   \n",
       "\n",
       "                                            Deepseek Humano  \n",
       "0  **Fecha apertura:** 06/01/2021 08:08  \\n**Fech...      -  \n",
       "1  **Fecha apertura:** 18/03/2021 10:28  \\n**Fech...      -  \n",
       "2  **Fecha:** 18/03/2021 15:18  \\n**Ubicación:** ...      -  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('Comparative_IA.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c155f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "\n",
    "df = pd.read_excel(\"Comparative_IA.xlsx\")\n",
    "\n",
    "# Using columns in a program\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Define parameters\n",
    "referencias = df[\"Original\"]\n",
    "modelos = [\"Gemini\", \"GPT - 4\", \"Deepseek\", \"Humano\"]\n",
    "\n",
    "# ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Function to apply ROUGE\n",
    "def compute_rouge_scores(referencias, predicciones):\n",
    "    r1, r2, rl = [], [], []\n",
    "    for ref, pred in zip(referencias, predicciones):\n",
    "        scores = scorer.score(str(ref), str(pred))\n",
    "        r1.append(scores['rouge1'].fmeasure)\n",
    "        r2.append(scores['rouge2'].fmeasure)\n",
    "        rl.append(scores['rougeL'].fmeasure)\n",
    "    return r1, r2, rl\n",
    "\n",
    "# Calculate metrics for each model\n",
    "for modelo in modelos:\n",
    "    preds = df[modelo]\n",
    "    \n",
    "    # ROUGE\n",
    "    r1, r2, rl = compute_rouge_scores(referencias, preds)\n",
    "    df[f\"{modelo}_rouge1\"] = r1\n",
    "    df[f\"{modelo}_rouge2\"] = r2\n",
    "    df[f\"{modelo}_rougeL\"] = rl\n",
    "\n",
    "    # BERTScore\n",
    "    P, R, F1 = bert_score.score(preds.astype(str).tolist(), referencias.astype(str).tolist(), lang=\"es\", verbose=False)\n",
    "    df[f\"{modelo}_bertscore_F1\"] = F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c85a5fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No metrics found! Please run the notebook first to calculate ROUGE and BERTScore metrics.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('Comparative_IA.xlsx')\n",
    "\n",
    "# Check if metrics have been calculated\n",
    "metric_columns = [col for col in df.columns if any(metric in col for metric in ['rouge1', 'rouge2', 'rougeL', 'bertscore'])]\n",
    "\n",
    "if not metric_columns:\n",
    "    print(\"❌ No metrics found! Please run the notebook first to calculate ROUGE and BERTScore metrics.\")\n",
    "else:\n",
    "    print(\"✅ Metrics found!\")\n",
    "    print(f\"📊 Found {len(metric_columns)} metric columns\")\n",
    "    \n",
    "    # Define models\n",
    "    modelos = [\"Gemini\", \"GPT - 4\", \"Deepseek\", \"Humano\"]\n",
    "    \n",
    "    # Display sample metrics\n",
    "    print(f\"\\n📈 Sample metrics (first 3 rows):\")\n",
    "    print(df[metric_columns].head(3))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 SUMMARY STATISTICS BY MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for modelo in modelos:\n",
    "        print(f\"\\n🤖 {modelo.upper()}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # ROUGE metrics\n",
    "        rouge1_mean = df[f\"{modelo}_rouge1\"].mean()\n",
    "        rouge2_mean = df[f\"{modelo}_rouge2\"].mean()\n",
    "        rougeL_mean = df[f\"{modelo}_rougeL\"].mean()\n",
    "        \n",
    "        # BERTScore\n",
    "        bertscore_mean = df[f\"{modelo}_bertscore_F1\"].mean()\n",
    "        \n",
    "        print(f\"ROUGE-1:  {rouge1_mean:.4f}\")\n",
    "        print(f\"ROUGE-2:  {rouge2_mean:.4f}\")\n",
    "        print(f\"ROUGE-L:  {rougeL_mean:.4f}\")\n",
    "        print(f\"BERTScore: {bertscore_mean:.4f}\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🏆 MODEL COMPARISON TABLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for modelo in modelos:\n",
    "        comparison_data.append({\n",
    "            'Model': modelo,\n",
    "            'ROUGE-1': df[f\"{modelo}_rouge1\"].mean(),\n",
    "            'ROUGE-2': df[f\"{modelo}_rouge2\"].mean(),\n",
    "            'ROUGE-L': df[f\"{modelo}_rougeL\"].mean(),\n",
    "            'BERTScore': df[f\"{modelo}_bertscore_F1\"].mean()\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.round(4)\n",
    "    \n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Highlight best scores\n",
    "    print(\"\\n🥇 BEST PERFORMANCE BY METRIC:\")\n",
    "    for metric in ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BERTScore']:\n",
    "        best_model = comparison_df.loc[comparison_df[metric].idxmax(), 'Model']\n",
    "        best_score = comparison_df[metric].max()\n",
    "        print(f\"{metric}: {best_model} ({best_score:.4f})\")\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\n💾 Saving results...\")\n",
    "    df.to_excel('Comparative_IA_with_metrics.xlsx', index=False)\n",
    "    comparison_df.to_excel('Model_Comparison_Summary.xlsx', index=False)\n",
    "    print(\"✅ Results saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc_experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
